{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict future sales.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jliVqACtM5ZZ"
      },
      "source": [
        "import gc\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import warnings\n",
        "import lightgbm as lgbm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YnRRNKwNPrB"
      },
      "source": [
        "api_token = {\"username\":\"watsons\",\"key\":\"dc7da47ca9aa5e696b65f97c16fd627b\"}\n",
        "if not os.path.exists(\"/root/.kaggle\"):\n",
        "    os.makedirs(\"/root/.kaggle\")\n",
        " \n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        " \n",
        "if not os.path.exists(\"/kaggle\"):\n",
        "    os.makedirs(\"/kaggle\")\n",
        "os.chdir('/kaggle')\n",
        "!kaggle competitions download -c competitive-data-science-predict-future-sales\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xZaHNbgM5Za"
      },
      "source": [
        "def reduce_mem_usage(df, silent=True, allow_categorical=True, float_dtype=\"float32\"):\n",
        "  def _downcast_numeric(series, allow_categorical=allow_categorical):\n",
        "    if pd.api.types.is_sparse(series.dtype) is True:\n",
        "      return series\n",
        "    elif pd.api.types.is_numeric_dtype(series.dtype) is False:\n",
        "      if pd.api.types.is_datetime64_any_dtype(series.dtype):\n",
        "        return series\n",
        "      else:\n",
        "        if allow_categorical:\n",
        "          return series\n",
        "        else:\n",
        "          codes, uniques = series.factorize()\n",
        "          series = pd.Series(data=codes, index=series.index)\n",
        "          series = _downcast_numeric(series)\n",
        "          return series\n",
        "      else:\n",
        "        series = pd.to_numeric(series, downcast=\"integer\")\n",
        "      if pd.api.types.is_float_dtype(series.dtype):\n",
        "        series = series.astype(float_dtype)\n",
        "      return series\n",
        "\n",
        "  if silent is False:\n",
        "    start_mem = np.sum(df.memory_usage()) / 1024 ** 2\n",
        "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
        "  if df.ndim == 1:\n",
        "    df = _downcast_numeric(df)\n",
        "  else:\n",
        "    for col in df.columns:\n",
        "      df.loc[:, col] = _downcast_numeric(df.loc[:,col])\n",
        "  if silent is False:\n",
        "    end_mem = np.sum(df.memory_usage()) / 1024 ** 2\n",
        "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
        "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
        "  return df\n",
        "\n",
        "def shrink_mem_new_cols(matrix, oldcols=None, allow_categorical=False):\n",
        "  if oldcols is not None:\n",
        "    newcols = matrix.columns.difference(oldcols)\n",
        "  else:\n",
        "    newcols = matrix.columns\n",
        "  matrix.loc[:,newcols] = reduce_mem_usage(matrix.loc[:,newcols], allow_categorical=allow_categorical)\n",
        "  oldcols = matrix.columns  # This is used to track which columns have already been downcast\n",
        "  return matrix, oldcols"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLIYLVhtM5Zb"
      },
      "source": [
        "items = pd.read_csv(\"items.csv\")\n",
        "shops = pd.read_csv(\"shops.csv\")\n",
        "train = pd.read_csv(\"sales_train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhHtnghaM5Zd"
      },
      "source": [
        "train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")\n",
        "train = train.loc[train.shop_id.isin(test[\"shop_id\"].unique()), :]\n",
        "train = train[(train[\"item_price\"] > 0) & (train[\"item_price\"] < 50000)]\n",
        "train = train[train[\"item_cnt_day\"] < 1000]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isPdBBGSzVg1"
      },
      "source": [
        "def create_testlike_train(sales_train, test=None):\n",
        "  indexlist = []\n",
        "  for i in sales_train.date_block_num.unique():\n",
        "    x = itertools.product([i],\n",
        "      sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n",
        "      sales_train.loc[sales_train.date_block_num == i].item_id.unique())\n",
        "    indexlist.append(np.array(list(x)))\n",
        "  df = pd.DataFrame(data=np.concatenate(indexlist,axis=0),columns=[\"date_block_num\", \"shop_id\", \"item_id\"],)\n",
        "\n",
        "  sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n",
        "  sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n",
        "    item_cnt_month = pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n",
        "    item_revenue_month = pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n",
        "    item_cnt_std = pd.NamedAgg(column=\"item_cnt_day\", aggfunc=np.std),\n",
        "    item_cnt_count = pd.NamedAgg(column=\"item_cnt_day\", aggfunc='count')\n",
        "    )\n",
        "\n",
        "  df = df.merge(sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],)\n",
        "\n",
        "  if test is not None:\n",
        "    test[\"date_block_num\"] = 34\n",
        "    test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n",
        "    test[\"shop_id\"] = test.shop_id.astype(np.int8)\n",
        "    test[\"item_id\"] = test.item_id.astype(np.int16)\n",
        "    test = test.drop(columns=\"ID\")\n",
        "\n",
        "    df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n",
        "\n",
        "  df.item_cnt_month = df.item_cnt_month.fillna(0)\n",
        "  df.item_revenue_month = df.item_revenue_month.fillna(0)\n",
        "  df.item_cnt_std = df.item_cnt_std.fillna(0)\n",
        "  df.item_cnt_count = df.item_cnt_count.fillna(0)\n",
        "  return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOM05zk0M5Zf"
      },
      "source": [
        "matrix = create_testlike_train(train, test)\n",
        "matrix = matrix.merge(items[['item_id', 'item_category_id']], how='left', on='item_id',)\n",
        "oldcols = matrix.columns\n",
        "matrix = reduce_mem_usage(matrix, silent=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MW1EMM9M5Zi"
      },
      "source": [
        "def add_time_features(m, train):\n",
        "  dummies = m.loc[m.date_block_num == 34, [\"date_block_num\", \"shop_id\", \"item_id\"]]\n",
        "  dummies = dummies.assign(date=pd.to_datetime(\"2015-11-30\"), item_price=1, item_cnt_day=0, item_revenue_day=0,)\n",
        "  train = pd.concat([train, dummies])\n",
        "  del dummies\n",
        "\n",
        "  month_last_day = train.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n",
        "  month_last_day[~month_last_day.dt.is_month_end] = (month_last_day[~month_last_day.dt.is_month_end] + MonthEnd())\n",
        "  month_first_day = train.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n",
        "  month_first_day[~month_first_day.dt.is_month_start] = (month_first_day[~month_first_day.dt.is_month_start] - MonthBegin())\n",
        "  month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n",
        "  m = m.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
        "  m = m.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
        "  \n",
        "  def last_sale_days(matrix):\n",
        "    last_shop_item_dates = []\n",
        "    for dbn in range(1, 35):\n",
        "      lsid_temp = (train.query(f\"date_block_num<{dbn}\").groupby([\"shop_id\", \"item_id\"]).date.max()\n",
        "      .rename(\"last_shop_item_sale_date\").reset_index())\n",
        "      lsid_temp[\"date_block_num\"] = dbn\n",
        "      last_shop_item_dates.append(lsid_temp)\n",
        "\n",
        "    last_shop_item_dates = pd.concat(last_shop_item_dates)\n",
        "    matrix = matrix.merge(last_shop_item_dates, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\")\n",
        "\n",
        "    def days_since_last_feat(m, feat_name, date_feat_name, missingval):\n",
        "      m[feat_name] = (m[\"month_first_day\"] - m[date_feat_name]).dt.days\n",
        "      m.loc[m[feat_name] > 2000, feat_name] = missingval\n",
        "      m.loc[m[feat_name].isna(), feat_name] = missingval\n",
        "      m['first_buy'] = 0\n",
        "      m.loc[m[feat_name] == missingval, 'first_buy'] = 1\n",
        "      return m\n",
        "\n",
        "    matrix = days_since_last_feat(matrix, \"last_shop_item_sale_days\", \"last_shop_item_sale_date\", 9999)\n",
        "\n",
        "    return matrix\n",
        "\n",
        "  savelist = []\n",
        "  for dbn in range(35):\n",
        "    item_amt = len(train[train['date_block_num'] == dbn].groupby(['item_id']))\n",
        "    shop_item_amt = len(train[train['date_block_num'] == dbn].groupby(['shop_id', 'item_id']))\n",
        "    savelist.append([dbn, item_amt, shop_item_amt])\n",
        "  df_right = pd.DataFrame(np.array(savelist), columns=['date_block_num', 'item_amt', 'shop_item_amt'])\n",
        "  \n",
        "  m = m.merge(df_right, how='left', on='date_block_num')\n",
        "  m = last_sale_days(m)\n",
        "  \n",
        "  m[\"month\"] = m[\"month_first_day\"].dt.month\n",
        "  m[\"year\"] = m[\"month_first_day\"].dt.year\n",
        "  m.drop(['month_first_day'], axis=1, inplace=True)\n",
        "  m[\"item_age\"] = m.groupby(\"item_id\")[\"date_block_num\"].transform(lambda x: x - x.min())\n",
        "  m[\"new_item\"] = m[\"item_age\"] == 0\n",
        "  m[\"new_item\"] = m[\"new_item\"].astype(\"int8\")\n",
        "  replace_val = m['last_shop_item_sale_days'][m['last_shop_item_sale_days'] != 9999].mean()\n",
        "  m['last_shop_item_sale_days'].replace(9999, replace_val, inplace=True)\n",
        "\n",
        "  return m"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3I0uXifM5Zl"
      },
      "source": [
        "matrix = add_time_features(matrix, train)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7UR6_owM5Zn"
      },
      "source": [
        "def add_price_features(matrix, train):\n",
        "  price_features = pd.DataFrame(train.groupby([\"date_block_num\", \"item_id\"]).item_price.mean()).reset_index()\n",
        "  price_features = price_features.merge(items[[\"item_id\", \"item_category_id\"]], how=\"left\", on=\"item_id\")\n",
        "  price_features[\"norm_diff_cat_price\"] = (price_features.groupby([\"date_block_num\", \"item_category_id\"])[\"item_price\"]\n",
        "  .transform(lambda x: round((x - x.mean()) / x.mean(),3)))\n",
        "  price_features = price_features[[\"date_block_num\", \"item_id\", \"item_price\", \"norm_diff_cat_price\",]]\n",
        "\n",
        "  features = [\"item_price\", \"norm_diff_cat_price\"]\n",
        "  aggs = {f: \"last\" for f in features}\n",
        "  renames = {f: \"last_\" + f for f in features}\n",
        "  features = []\n",
        "  for dbn in range(1, 35):\n",
        "    f_temp = (price_features.query(f\"date_block_num<{dbn}\").groupby(\"item_id\").agg(aggs).rename(columns=renames))\n",
        "    f_temp[\"date_block_num\"] = dbn\n",
        "    features.append(f_temp)\n",
        "  features = pd.concat(features).reset_index()\n",
        "  matrix = matrix.merge(features, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n",
        "  return matrix"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyzwwD1nM5Zn"
      },
      "source": [
        "matrix = add_price_features(matrix, train)\n",
        "matrix.drop(['last_shop_item_sale_date'], axis=1, inplace=True)\n",
        "matrix[['month_length','year']] = OrdinalEncoder().fit_transform(matrix[['month_length','year']])\n",
        "del train,test,items,shops\n",
        "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD3r9r1dM5Zq"
      },
      "source": [
        "def add_pct_change(matrix, group_feats, target=\"item_cnt_month\", aggfunc=\"mean\", periods=[1], lag=1, clip_value=None):\n",
        "  dat = matrix.pivot_table(index=group_feats + [\"date_block_num\"], values=target, aggfunc=aggfunc, fill_value=0, dropna=False,).astype(\"float32\")\n",
        "  for g in group_feats:\n",
        "    firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
        "    dat = dat.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
        "    dat.loc[dat.index.get_level_values(\"date_block_num\") < dat[\"firsts\"], target] = float(\"nan\")\n",
        "    del dat[\"firsts\"]\n",
        "\n",
        "  for period in periods:\n",
        "    feat_name = \"_\".join(group_feats + [target] + [aggfunc] + [\"delta\"] + [str(period)] + [f\"lag_{lag}\"])\n",
        "    dat = dat.groupby(group_feats)[target].transform(lambda x: x.pct_change(periods=period, fill_method=\"pad\")).rename(feat_name)\n",
        "    if clip_value is not None:\n",
        "      dat = dat.clip(lower=-clip_value, upper=clip_value)\n",
        "\n",
        "  dat = dat.reset_index()\n",
        "  dat[\"date_block_num\"] += lag\n",
        "  matrix = matrix.merge(dat, on=[\"date_block_num\"] + group_feats, how=\"left\")\n",
        "  matrix[feat_name] = reduce_mem_usage(matrix[feat_name])\n",
        "  return matrix"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWIevAhiM5Zr"
      },
      "source": [
        "matrix = add_pct_change(matrix, [\"item_id\"], \"item_cnt_month\", clip_value=3)\n",
        "matrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", clip_value=3)\n",
        "matrix = add_pct_change(matrix, [\"shop_id\"], \"item_cnt_month\", clip_value=3)\n",
        "matrix = add_pct_change(matrix, [\"item_category_id\",'shop_id'], \"item_cnt_month\", clip_value=3)\n",
        "matrix = add_pct_change(matrix, [\"shop_id\",'item_id'], \"item_cnt_month\", clip_value=3)\n",
        "'''\n",
        "matrix = add_pct_change(matrix, [\"item_id\"], \"item_cnt_count\", clip_value=3)\n",
        "matrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_count\", clip_value=3)\n",
        "matrix = add_pct_change(matrix, [\"shop_id\"], \"item_cnt_count\", clip_value=3)\n",
        "matrix = add_pct_change(matrix, [\"item_category_id\",'shop_id'], \"item_cnt_count\", clip_value=3)\n",
        "matrix = add_pct_change(matrix, [\"shop_id\",'item_id'], \"item_cnt_count\", clip_value=3)\n",
        "'''\n",
        "matrix = add_pct_change(matrix, ['item_id','shop_id'], \"last_item_price\", clip_value=3, lag=0)\n",
        "matrix = add_pct_change(matrix, [\"item_id\"], \"last_norm_diff_cat_price\", clip_value=3, lag=0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyexT0cVM5Zr"
      },
      "source": [
        "matrix, oldcols= shrink_mem_new_cols(matrix, oldcols)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uN5YoUDM5Zr"
      },
      "source": [
        "def rolling_stat(matrix, feats, window, argfeat=\"item_cnt_month\", aggfunc=\"mean\"):\n",
        "  feat_name = '_'.join(feats + [argfeat] + [str(window)] + [aggfunc] + [argfeat])\n",
        "  source = matrix\n",
        "  store = []\n",
        "  for i in range(2,35):\n",
        "    mes = (source[source.date_block_num.isin(range(max([i - window, 0]), i))]\n",
        "    .groupby(feats)[argfeat].agg(aggfunc).astype('float64').rename(feat_name).reset_index())\n",
        "    mes[\"date_block_num\"] = i\n",
        "    store.append(mes)\n",
        "  store = pd.concat(store)\n",
        "  matrix = matrix.merge(store, on=feats + [\"date_block_num\"], how=\"left\")\n",
        "  return matrix"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot0zt6fEM5Zs"
      },
      "source": [
        "windowlist = [1,12]\n",
        "for window in windowlist:\n",
        "  matrix = rolling_stat(matrix,[\"shop_id\",'item_id'],window=window)\n",
        "  matrix = rolling_stat(matrix,[\"shop_id\",'item_category_id'],window=window)\n",
        "  matrix = rolling_stat(matrix,[\"shop_id\"],window=window)\n",
        "  matrix = rolling_stat(matrix,['item_category_id'],window=window)\n",
        "  matrix = rolling_stat(matrix,['item_id'],window=window)\n",
        "\n",
        "  matrix = rolling_stat(matrix,[\"shop_id\",'item_id'],window=window,argfeat=\"item_revenue_month\")\n",
        "  matrix = rolling_stat(matrix,[\"shop_id\",'item_category_id'],window=window,argfeat=\"item_revenue_month\")\n",
        "  matrix = rolling_stat(matrix,[\"shop_id\"],window=window,argfeat=\"item_revenue_month\")\n",
        "  matrix = rolling_stat(matrix,['item_category_id'],window=window,argfeat=\"item_revenue_month\")\n",
        "  matrix = rolling_stat(matrix,['item_id'],window=window,argfeat=\"item_revenue_month\")\n",
        "  \n",
        "  matrix = rolling_stat(matrix,[\"shop_id\",'item_id'],window=window,argfeat=\"item_cnt_count\")\n",
        "  matrix = rolling_stat(matrix,[\"shop_id\",'item_category_id'],window=window,argfeat=\"item_cnt_count\")\n",
        "  matrix = rolling_stat(matrix,[\"shop_id\"],window=window,argfeat=\"item_cnt_count\")\n",
        "  matrix = rolling_stat(matrix,['item_category_id'],window=window,argfeat=\"item_cnt_count\")\n",
        "  matrix = rolling_stat(matrix,['item_id'],window=window,argfeat=\"item_cnt_count\")\n",
        "\n",
        "  matrix = rolling_stat(matrix,[\"shop_id\",'item_id'],window=window,argfeat=\"item_cnt_std\")\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2ZvT7p-VAdF"
      },
      "source": [
        "catlist = ['item_cnt_month', 'month_length', 'first_buy', 'month', 'year', 'new_item', \"shop_id\", \"item_id\", \"item_category_id\", 'date_block_num']\n",
        "conlist = list(set(matrix.columns).difference(set(catlist)))\n",
        "\n",
        "for col in conlist:\n",
        "  matrix[col] = MinMaxScaler().fit_transform(np.array(matrix[col]).reshape(-1,1))\n",
        "gc.collect()\n",
        "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)\n",
        "matrix.to_pickle(\"checkpoint_final.pkl\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSJpzH5FM5aG"
      },
      "source": [
        "def fit_booster(X_train, y_train, X_test=None, y_test=None,categoricals=[], test_run=False, early_stopping=True):\n",
        "    params = {\n",
        "    \"min_child_samples\": 30,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"max_depth\":4,\n",
        "    \"colsample_bytree\": 0.6,\n",
        "    \"subsample\": 0.6,\n",
        "    \"n_estimators\": 200,\n",
        "    }\n",
        "\n",
        "    early_stopping_rounds = None\n",
        "    if early_stopping == True:\n",
        "        early_stopping_rounds = 50\n",
        "\n",
        "    if test_run:\n",
        "        eval_set = [(X_train, y_train)]\n",
        "    else:\n",
        "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
        "\n",
        "    booster = lgbm.LGBMRegressor(**params)\n",
        "\n",
        "\n",
        "    booster.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        eval_set=eval_set,\n",
        "        eval_metric=[\"rmse\"],\n",
        "        verbose=100,\n",
        "        categorical_feature=categoricals,\n",
        "        early_stopping_rounds=early_stopping_rounds,\n",
        "    )\n",
        "\n",
        "    return booster"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LskuKu4vM5aH"
      },
      "source": [
        "matrix = pd.read_pickle(\"checkpoint_final.pkl\")\n",
        "categoricals = ['month_length', 'first_buy', 'month', 'year', 'new_item', \"item_category_id\", 'date_block_num']\n",
        "keep_from_month = 2 \n",
        "test_month = 33\n",
        "dropcols = [\"shop_id\", \"item_id\", 'item_cnt_std', 'item_cnt_count', 'item_revenue_month']  # The features are dropped to reduce overfitting\n",
        "\n",
        "valid = matrix.drop(columns=dropcols).loc[matrix.date_block_num == test_month, : ]\n",
        "train = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, : ]\n",
        "train = train[train.date_block_num >= keep_from_month + 4]\n",
        "X_train = train.drop(columns=\"item_cnt_month\")\n",
        "y_train = train.item_cnt_month\n",
        "X_valid = valid.drop(columns=\"item_cnt_month\")\n",
        "y_valid = valid.item_cnt_month\n",
        "\n",
        "gc.collect()\n",
        "del (valid, train, matrix)\n",
        "lgbooster = fit_booster(X_train,y_train,X_valid,y_valid,categoricals=categoricals)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7YMphacM5aJ"
      },
      "source": [
        "#_ = lgbm.plot_importance(lgbooster, figsize=(10,50), height=0.7, importance_type=\"gain\", max_num_features=50)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO5hjUmAM5aN"
      },
      "source": [
        "matrix = pd.read_pickle(\"checkpoint_final.pkl\")\n",
        "\n",
        "test_month = 34\n",
        "train = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, : ]\n",
        "train = train[train.date_block_num >= keep_from_month + 4]\n",
        "X_train = train.drop(columns=\"item_cnt_month\")\n",
        "y_train = train.item_cnt_month\n",
        "X_test = matrix.loc[matrix.date_block_num == test_month, : ]\n",
        "X_test = X_test.drop(columns=\"item_cnt_month\")\n",
        "del train,matrix\n",
        "lgbooster = fit_booster(X_train,y_train,categoricals=categoricals,test_run=True, early_stopping=None)\n",
        "X_test[\"item_cnt_month\"] = lgbooster.predict(X_test.drop(columns=dropcols)).clip(0,20)\n",
        "test_orig = pd.read_csv(\"test.csv\")\n",
        "test = test_orig.merge(\n",
        "    X_test[[\"shop_id\", \"item_id\", \"item_cnt_month\"]],\n",
        "    on=[\"shop_id\", \"item_id\"],\n",
        "    how=\"inner\",\n",
        "    copy=True,\n",
        ")\n",
        "assert test_orig.equals(test[[\"ID\", \"shop_id\", \"item_id\"]])\n",
        "test[[\"ID\", \"item_cnt_month\"]].to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZipwDRPePPyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded293b4-ae72-42ea-9202-e84165050b0d"
      },
      "source": [
        "!kaggle competitions submit -c competitive-data-science-predict-future-sales -f submission.csv -m \"Message\""
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "100% 5.34M/5.34M [00:00<00:00, 12.0MB/s]\n",
            "400 - Bad Request\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}